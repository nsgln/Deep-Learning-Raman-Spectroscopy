{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confident-stationery",
   "metadata": {},
   "source": [
    "# Fine-tuning experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-evolution",
   "metadata": {},
   "source": [
    "Based on the two notebooks : https://github.com/csho33/bacteria-ID/blob/master/1_reference_finetuning.ipynb & https://github.com/csho33/bacteria-ID/blob/master/3_clinical_finetuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "reduced-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t00 = time()\n",
    "import numpy as np\n",
    "import os,sys,re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-arrow",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "promotional-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from directory\n",
    "os.chdir(os.getcwd())\n",
    "base_dir = 'Raman_Data/'\n",
    "als_dir = base_dir + 'ALS/'\n",
    "ctrl_dir = base_dir + 'CTRL/'\n",
    "\n",
    "base_dir2 = 'Bacteria_TL'\n",
    "sys.path.append(base_dir2)\n",
    "\n",
    "models = ['pretrained_model.ckpt', 'finetuned_model.ckpt', 'clinical_pretrained_model.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "annoying-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_als = os.listdir(als_dir)\n",
    "all_files_als.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "\n",
    "all_files_ctrl = os.listdir(ctrl_dir)\n",
    "all_files_ctrl.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "capable-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(file, dir):\n",
    "    with open(dir + file, 'rt') as fd:\n",
    "        data=[]\n",
    "        line = fd.readline()\n",
    "        nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "        data.append(nline)\n",
    "        while line:\n",
    "            line=fd.readline()\n",
    "            nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "            data.append(nline)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "freelance-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[] #actual y of spectra\n",
    "Y=[] # 1 -> als; 0 -> ctrl\n",
    "coord=[] #actual x of spectra\n",
    "\n",
    "sep=[60,78,114,150,194,210,225,241,255,280,299,313,323,333,343,353,363,373,383,393] #Il manque le 227\n",
    "groups=[] #for GROUP K FOLD\n",
    "group=0\n",
    "index=1\n",
    "for f in all_files_als:\n",
    "    data=[]\n",
    "    datab=[]\n",
    "    for e in parse_text(f, als_dir):\n",
    "        if len(e) > 0:\n",
    "            datab.append(float(e[0]))\n",
    "            data.append(float(e[1]))\n",
    "    coord.append(datab)\n",
    "    X.append(data)\n",
    "    Y.append(1)\n",
    "    groups.append(group)\n",
    "    if index in sep:\n",
    "        group+=1\n",
    "    index+=1\n",
    "    \n",
    "sep=[33,76,91,138,149,158,168,178,188,198]\n",
    "index=1\n",
    "for f in all_files_ctrl:\n",
    "    data=[]\n",
    "    datab=[]\n",
    "    for e in parse_text(f, ctrl_dir):\n",
    "        if len(e) > 0:\n",
    "            datab.append(float(e[0]))\n",
    "            data.append(float(e[1]))\n",
    "    coord.append(datab)\n",
    "    X.append(data)\n",
    "    Y.append(0)\n",
    "    groups.append(group)\n",
    "    if index in sep:\n",
    "        group+=1\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "occasional-persian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 1174) (591,)\n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "groups=np.array(groups)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "swedish-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range (len(X[i])):\n",
    "        if(X[i][j] < 0):\n",
    "            X[i][j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-benefit",
   "metadata": {},
   "source": [
    "## Split our dataset into a finetunable set and a full test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-bones",
   "metadata": {},
   "source": [
    "First split 2/3 -> 20 patients for finetune and 10 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "funded-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [23, 15, 20, 0, 24, 28, 16, 5, 1, 26, 29, 7, 12, 2, 3, 8, 27, 22, 11, 6]\n",
      "10 [10, 14, 9, 19, 13, 18, 21, 25, 17, 4]\n"
     ]
    }
   ],
   "source": [
    "patient_idxs_finetune = []\n",
    "\n",
    "x = list(range(0, 20))\n",
    "patient_idxs_finetune = random.sample(x,12)\n",
    "patient_idxs_test = [i for i in x if i not in patient_idxs_finetune]\n",
    "\n",
    "x2 = list(range(20, 30))\n",
    "patient_idxs_finetune += random.sample(x2,8)\n",
    "patient_idxs_test += [i for i in x2 if i not in patient_idxs_finetune]\n",
    "\n",
    "\n",
    "random.shuffle(patient_idxs_finetune)\n",
    "random.shuffle(patient_idxs_test)\n",
    "\n",
    "print(len(patient_idxs_finetune), patient_idxs_finetune)\n",
    "print(len(patient_idxs_test), patient_idxs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-candle",
   "metadata": {},
   "source": [
    "## Load ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "positive-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "artistic-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "layers = 6\n",
    "hidden_size = 100\n",
    "block_size = 2\n",
    "hidden_sizes = [hidden_size] * layers\n",
    "num_blocks = [block_size] * layers\n",
    "input_dim = 1174\n",
    "in_channels = 64\n",
    "n_classes = 2 # instead of 30, we use the 2 empiric groupings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "effective-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove last layers\n",
    "def removekey(d, listofkeys):\n",
    "    r = dict(d)\n",
    "    for key in listofkeys:\n",
    "        print('key: {} is removed'.format(key))\n",
    "        r.pop(key)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "worth-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n",
    "                    in_channels=in_channels, n_classes=n_classes)\n",
    "    if cuda: cnn.cuda()\n",
    "\n",
    "    checkpoint = torch.load(base_dir2 + '/' + models[1], map_location=lambda storage, loc: storage)\n",
    "    mod_weights = removekey(checkpoint, ['linear.weight', 'linear.bias'])\n",
    "    cnn.load_state_dict(mod_weights, strict=False)\n",
    "    return cnn, mod_weights, checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-classics",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-prerequisite",
   "metadata": {},
   "source": [
    "### Custom Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "nervous-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 29, 23, 11, 1], [12, 2, 8, 7, 24], [5, 26, 28, 0, 20], [22, 15, 6, 27, 16]]\n"
     ]
    }
   ],
   "source": [
    "patient_idxs = []\n",
    "x = patient_idxs_finetune\n",
    "for i in range(4):\n",
    "    l = random.sample(x,5)\n",
    "    patient_idxs.append(l)\n",
    "    x = [i for i in x if i not in l]\n",
    "print(patient_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "corrected-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 patients\n",
      " Tr: [3, 29, 23]\n",
      " Val: 11\n",
      " Te : 1\n",
      "3 [114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149] 36\n",
      "29 [581, 582, 583, 584, 585, 586, 587, 588, 589, 590] 10\n",
      "23 [484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530] 47\n",
      "11 [299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312] 14\n",
      "1 [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77] 18\n",
      "Group 1 patients\n",
      " Tr: [12, 2, 8]\n",
      " Val: 7\n",
      " Te : 24\n",
      "12 [313, 314, 315, 316, 317, 318, 319, 320, 321, 322] 10\n",
      "2 [78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113] 36\n",
      "8 [241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254] 14\n",
      "7 [225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240] 16\n",
      "24 [531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541] 11\n",
      "Group 2 patients\n",
      " Tr: [5, 26, 28]\n",
      " Val: 0\n",
      " Te : 20\n",
      "5 [194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209] 16\n",
      "26 [551, 552, 553, 554, 555, 556, 557, 558, 559, 560] 10\n",
      "28 [571, 572, 573, 574, 575, 576, 577, 578, 579, 580] 10\n",
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59] 60\n",
      "20 [393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425] 33\n",
      "Group 3 patients\n",
      " Tr: [22, 15, 6]\n",
      " Val: 27\n",
      " Te : 16\n",
      "22 [469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483] 15\n",
      "15 [343, 344, 345, 346, 347, 348, 349, 350, 351, 352] 10\n",
      "6 [210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224] 15\n",
      "27 [561, 562, 563, 564, 565, 566, 567, 568, 569, 570] 10\n",
      "16 [353, 354, 355, 356, 357, 358, 359, 360, 361, 362] 10\n"
     ]
    }
   ],
   "source": [
    "# Sample train/val/te spectra\n",
    "idx_tr, idx_val, idx_te = [], [], []\n",
    "for group_idx, patient_list in enumerate(patient_idxs):\n",
    "    print('Group {} patients'.format(group_idx))\n",
    "    print(' Tr: {}'.format(patient_list[:3]))\n",
    "    print(' Val: {}'.format(patient_list[3]))\n",
    "    print(' Te : {}'.format(patient_list[4]))\n",
    "    for j, patient in enumerate(patient_list):\n",
    "        l= np.where(groups == patient)\n",
    "        start_idx = l[0][0]\n",
    "        end_idx = l[0][len(l[0])-1]\n",
    "        idx_range = list(range(start_idx, end_idx+1))\n",
    "        print(patient, idx_range, len(idx_range))\n",
    "        np.random.shuffle(idx_range)\n",
    "        #idx_sample = idx_range[:5]\n",
    "        if j < 3:\n",
    "            idx_tr.extend(idx_range)\n",
    "        elif j ==3:\n",
    "            idx_val.extend(idx_range)\n",
    "        else:\n",
    "            idx_te.extend(idx_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "qualified-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "100\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_tr))\n",
    "print(len(idx_val))\n",
    "print(len(idx_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-bobby",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "dress-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import spectral_dataloader\n",
    "from training import run_epoch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "confidential-criminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 92.14\n",
      "  Val acc: 99.00\n",
      "  Test acc: 87.50\n",
      "Finished: 20.65s\n"
     ]
    }
   ],
   "source": [
    "cnn, _, _ = load_model()\n",
    "# Fine-tune CNN\n",
    "epochs = 1 # Change this number to ~30 for full training\n",
    "batch_size = 10\n",
    "t0 = time()\n",
    "# Set up Adam optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "# Set up dataloaders\n",
    "dl_tr = spectral_dataloader(X, Y, idxs=idx_tr,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "dl_val = spectral_dataloader(X, Y, idxs=idx_val,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "dl_te = spectral_dataloader(X, Y, idxs=idx_te,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Fine-tune CNN for first fold\n",
    "best_val = 0\n",
    "no_improvement = 0\n",
    "max_no_improvement = 5\n",
    "print('Starting fine-tuning!')\n",
    "for epoch in range(epochs):\n",
    "    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n",
    "    # Train\n",
    "    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n",
    "        training=True, optimizer=optimizer)\n",
    "    print('  Train acc: {:0.2f}'.format(acc_tr))\n",
    "    # Val\n",
    "    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n",
    "        training=False, optimizer=optimizer)\n",
    "    print('  Val acc: {:0.2f}'.format(acc_val))\n",
    "    # Test\n",
    "    acc_te, loss_te = run_epoch(epoch, cnn, dl_te, cuda,\n",
    "        training=False, optimizer=optimizer)\n",
    "    print('  Test acc: {:0.2f}'.format(acc_te))\n",
    "    # Check performance for early stopping\n",
    "    if acc_val > best_val or epoch == 0:\n",
    "        best_val = acc_val\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    if no_improvement >= max_no_improvement:\n",
    "        print('Finished after {} epochs!'.format(epoch+1))\n",
    "        break\n",
    "print('Finished: {:0.2f}s'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-involvement",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "behind-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import get_predictions\n",
    "from scipy import stats\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "fiscal-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting test indices\n",
    "idx_te = []\n",
    "for group_idx in patient_idxs_test:\n",
    "    l= np.where(groups == group_idx)\n",
    "    start_idx = l[0][0]\n",
    "    end_idx = l[0][len(l[0])-1]\n",
    "    idx_te += list(range(start_idx, end_idx+1))\n",
    "dl_te = spectral_dataloader(X, Y, idxs=idx_te,\n",
    "    batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "hawaiian-hughes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: 5.98s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# Make predictions on the 5 test patients\n",
    "y_hat = get_predictions(cnn, dl_te, cuda)\n",
    "print('Finished: {:0.2f}s'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "mounted-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_l = []\n",
    "for i in range(len(Y)):\n",
    "    if i in idx_te:\n",
    "        Y_l.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "roman-submission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.3%\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy\n",
    "acc = (y_hat == Y_l).mean()\n",
    "print('Accuracy: {:0.1f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "challenging-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_l)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-philosophy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
