{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbu3Ct6HA1fy"
   },
   "source": [
    "# Fine-tuning on the reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO1vDaQ4A7MU"
   },
   "source": [
    "Based on the first notebook : https://github.com/csho33/bacteria-ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXO6TrGhBQAu"
   },
   "source": [
    "## Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MEBRu7RX5NwT"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "t00 = time()\n",
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "import os, re, sys\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pd_49FKA_Rl"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yGnfrIMBFaO",
    "outputId": "2e1eb028-87e4-4092-ba2f-40287035e101"
   },
   "outputs": [],
   "source": [
    "#Load from Google Drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#root_dir = \"/content/drive/My Drive/\"\n",
    "#base_dir = root_dir + 'Raman_Data/'\n",
    "#base_dir2 = root_dir + 'Bacteria_TL'\n",
    "\n",
    "#Load from directory\n",
    "os.chdir(os.getcwd())\n",
    "base_dir = 'Raman_Data/'\n",
    "als_dir = base_dir + 'ALS/'\n",
    "ctrl_dir = base_dir + 'CTRL/'\n",
    "\n",
    "base_dir2 = 'Bacteria_TL'\n",
    "sys.path.append(base_dir2)\n",
    "#\n",
    "models = ['pretrained_model.ckpt', 'finetuned_model.ckpt', 'clinical_pretrained_model.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtwEPiqyBo8-",
    "outputId": "fe00e09d-2585-490c-a77b-1324a06d2b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "all_files_als = os.listdir(als_dir)\n",
    "print(len(all_files_als))\n",
    "all_files_ctrl = os.listdir(ctrl_dir)\n",
    "print(len(all_files_ctrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3L4oIupBuVp"
   },
   "source": [
    "Sort files by ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MBFptX6rBxIS"
   },
   "outputs": [],
   "source": [
    "all_files_als.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "all_files_ctrl.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg1xaAkZB27c"
   },
   "source": [
    "### Utility function to parse data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OWMxbzfVB2dJ"
   },
   "outputs": [],
   "source": [
    "def parse_text(file, dir):\n",
    "  with open(dir + file, 'rt') as fd:\n",
    "    data=[]\n",
    "    line = fd.readline()\n",
    "    nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "    data.append(nline)\n",
    "    while line:\n",
    "      line=fd.readline()\n",
    "      nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "      data.append(nline)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkqcDB0eCJzk"
   },
   "source": [
    "### Create X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jvBpmTnnCLjb"
   },
   "outputs": [],
   "source": [
    "X=[] #actual y of spectra\n",
    "Y=[] # 1 -> als; 0 -> ctrl\n",
    "coord=[] #actual x of spectra\n",
    "\n",
    "sep=[60,78,114,150,194,210,225,241,255,280,299,313,323,333,343,353,363,373,383,393] #Il manque le 227\n",
    "groups=[] #for GROUP K FOLD\n",
    "group=0\n",
    "index=1\n",
    "for f in all_files_als:\n",
    "  data=[]\n",
    "  datab=[]\n",
    "  for e in parse_text(f, als_dir):\n",
    "    if len(e) > 0:\n",
    "      datab.append(float(e[0]))\n",
    "      data.append(float(e[1]))\n",
    "  coord.append(datab)\n",
    "  X.append(data)\n",
    "  Y.append(1)\n",
    "  groups.append(group)\n",
    "  if index in sep:\n",
    "    group+=1\n",
    "  index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CZo4eHCBB7qb"
   },
   "outputs": [],
   "source": [
    "#print(len(X))\n",
    "sep=[33,76,91,138,149,158,168,178,188,198]\n",
    "index=1\n",
    "for f in all_files_ctrl:\n",
    "  data=[]\n",
    "  datab=[]\n",
    "  for e in parse_text(f, ctrl_dir):\n",
    "    if len(e) > 0:\n",
    "      datab.append(float(e[0]))\n",
    "      data.append(float(e[1]))\n",
    "  coord.append(datab)\n",
    "  X.append(data)\n",
    "  Y.append(0)\n",
    "  groups.append(group)\n",
    "  if index in sep:\n",
    "    group+=1\n",
    "  index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ytc35pqCQbF",
    "outputId": "acfc8500-e701-47b4-bf23-ba1bb4e1aed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 1174) (591,)\n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "groups=np.array(groups)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove negative values from spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range (len(X[i])):\n",
    "        if(X[i][j] < 0):\n",
    "            X[i][j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE-GuahDFSUh"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UL00DNXfFTzB"
   },
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform\n",
    "X_std = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PX-w-94SFVc-",
    "outputId": "bc242348-d68e-4e6c-9d11-4bacdfacc2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 500)\n"
     ]
    }
   ],
   "source": [
    "# Create a pca object with the 2 components as a parameter\n",
    "pca = decomposition.PCA(n_components=500)\n",
    "\n",
    "# Fit the PCA and transform the data\n",
    "X_std_pca = pca.fit_transform(X_std)\n",
    "print(X_std_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8J3B1hWCb18"
   },
   "source": [
    "## Loading pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cjBn_Wy1Cevl"
   },
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "layers = 6\n",
    "hidden_size = 100\n",
    "block_size = 2\n",
    "hidden_sizes = [hidden_size] * layers\n",
    "num_blocks = [block_size] * layers\n",
    "input_dim = 1174\n",
    "in_channels = 64\n",
    "n_classes = 2 # 2 classes -> 0 : ctrl & 1 : als\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1QXqHkh9PQWq"
   },
   "outputs": [],
   "source": [
    "def removekey(d, listofkeys):\n",
    "    r = dict(d)\n",
    "    for key in listofkeys:\n",
    "        print('key: {} is removed'.format(key))\n",
    "        r.pop(key)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MKH-h94Cujy",
    "outputId": "e0c16fd5-a1dd-4ed6-8a00-c01a07af95cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['linear.weight', 'linear.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from resnet import ResNet\n",
    "# Load trained weights for demo\n",
    "cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n",
    "                in_channels=in_channels, n_classes=n_classes)\n",
    "if cuda: cnn.cuda()\n",
    "\n",
    "## PROBLEM OF DIFFERING NUMBER OF CLASSES\n",
    "#cnn.load_state_dict(torch.load('./pretrained_model.ckpt', map_location=lambda storage, loc: storage))\n",
    "\n",
    "checkpoint = torch.load(base_dir2 + '/' + models[0], map_location=lambda storage, loc: storage)\n",
    "mod_weights = removekey(checkpoint, ['linear.weight', 'linear.bias'])\n",
    "cnn.load_state_dict(mod_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARAR34kFNWTy",
    "outputId": "d980b2b4-ba07-466a-96fe-e8d08890efe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "encoder.0.0.conv1.weight\n",
      "encoder.0.0.bn1.weight\n",
      "encoder.0.0.bn1.bias\n",
      "encoder.0.0.bn1.running_mean\n",
      "encoder.0.0.bn1.running_var\n",
      "encoder.0.0.conv2.weight\n",
      "encoder.0.0.bn2.weight\n",
      "encoder.0.0.bn2.bias\n",
      "encoder.0.0.bn2.running_mean\n",
      "encoder.0.0.bn2.running_var\n",
      "encoder.0.0.shortcut.0.weight\n",
      "encoder.0.0.shortcut.1.weight\n",
      "encoder.0.0.shortcut.1.bias\n",
      "encoder.0.0.shortcut.1.running_mean\n",
      "encoder.0.0.shortcut.1.running_var\n",
      "encoder.0.1.conv1.weight\n",
      "encoder.0.1.bn1.weight\n",
      "encoder.0.1.bn1.bias\n",
      "encoder.0.1.bn1.running_mean\n",
      "encoder.0.1.bn1.running_var\n",
      "encoder.0.1.conv2.weight\n",
      "encoder.0.1.bn2.weight\n",
      "encoder.0.1.bn2.bias\n",
      "encoder.0.1.bn2.running_mean\n",
      "encoder.0.1.bn2.running_var\n",
      "encoder.1.0.conv1.weight\n",
      "encoder.1.0.bn1.weight\n",
      "encoder.1.0.bn1.bias\n",
      "encoder.1.0.bn1.running_mean\n",
      "encoder.1.0.bn1.running_var\n",
      "encoder.1.0.conv2.weight\n",
      "encoder.1.0.bn2.weight\n",
      "encoder.1.0.bn2.bias\n",
      "encoder.1.0.bn2.running_mean\n",
      "encoder.1.0.bn2.running_var\n",
      "encoder.1.0.shortcut.0.weight\n",
      "encoder.1.0.shortcut.1.weight\n",
      "encoder.1.0.shortcut.1.bias\n",
      "encoder.1.0.shortcut.1.running_mean\n",
      "encoder.1.0.shortcut.1.running_var\n",
      "encoder.1.1.conv1.weight\n",
      "encoder.1.1.bn1.weight\n",
      "encoder.1.1.bn1.bias\n",
      "encoder.1.1.bn1.running_mean\n",
      "encoder.1.1.bn1.running_var\n",
      "encoder.1.1.conv2.weight\n",
      "encoder.1.1.bn2.weight\n",
      "encoder.1.1.bn2.bias\n",
      "encoder.1.1.bn2.running_mean\n",
      "encoder.1.1.bn2.running_var\n",
      "encoder.2.0.conv1.weight\n",
      "encoder.2.0.bn1.weight\n",
      "encoder.2.0.bn1.bias\n",
      "encoder.2.0.bn1.running_mean\n",
      "encoder.2.0.bn1.running_var\n",
      "encoder.2.0.conv2.weight\n",
      "encoder.2.0.bn2.weight\n",
      "encoder.2.0.bn2.bias\n",
      "encoder.2.0.bn2.running_mean\n",
      "encoder.2.0.bn2.running_var\n",
      "encoder.2.0.shortcut.0.weight\n",
      "encoder.2.0.shortcut.1.weight\n",
      "encoder.2.0.shortcut.1.bias\n",
      "encoder.2.0.shortcut.1.running_mean\n",
      "encoder.2.0.shortcut.1.running_var\n",
      "encoder.2.1.conv1.weight\n",
      "encoder.2.1.bn1.weight\n",
      "encoder.2.1.bn1.bias\n",
      "encoder.2.1.bn1.running_mean\n",
      "encoder.2.1.bn1.running_var\n",
      "encoder.2.1.conv2.weight\n",
      "encoder.2.1.bn2.weight\n",
      "encoder.2.1.bn2.bias\n",
      "encoder.2.1.bn2.running_mean\n",
      "encoder.2.1.bn2.running_var\n",
      "encoder.3.0.conv1.weight\n",
      "encoder.3.0.bn1.weight\n",
      "encoder.3.0.bn1.bias\n",
      "encoder.3.0.bn1.running_mean\n",
      "encoder.3.0.bn1.running_var\n",
      "encoder.3.0.conv2.weight\n",
      "encoder.3.0.bn2.weight\n",
      "encoder.3.0.bn2.bias\n",
      "encoder.3.0.bn2.running_mean\n",
      "encoder.3.0.bn2.running_var\n",
      "encoder.3.0.shortcut.0.weight\n",
      "encoder.3.0.shortcut.1.weight\n",
      "encoder.3.0.shortcut.1.bias\n",
      "encoder.3.0.shortcut.1.running_mean\n",
      "encoder.3.0.shortcut.1.running_var\n",
      "encoder.3.1.conv1.weight\n",
      "encoder.3.1.bn1.weight\n",
      "encoder.3.1.bn1.bias\n",
      "encoder.3.1.bn1.running_mean\n",
      "encoder.3.1.bn1.running_var\n",
      "encoder.3.1.conv2.weight\n",
      "encoder.3.1.bn2.weight\n",
      "encoder.3.1.bn2.bias\n",
      "encoder.3.1.bn2.running_mean\n",
      "encoder.3.1.bn2.running_var\n",
      "encoder.4.0.conv1.weight\n",
      "encoder.4.0.bn1.weight\n",
      "encoder.4.0.bn1.bias\n",
      "encoder.4.0.bn1.running_mean\n",
      "encoder.4.0.bn1.running_var\n",
      "encoder.4.0.conv2.weight\n",
      "encoder.4.0.bn2.weight\n",
      "encoder.4.0.bn2.bias\n",
      "encoder.4.0.bn2.running_mean\n",
      "encoder.4.0.bn2.running_var\n",
      "encoder.4.0.shortcut.0.weight\n",
      "encoder.4.0.shortcut.1.weight\n",
      "encoder.4.0.shortcut.1.bias\n",
      "encoder.4.0.shortcut.1.running_mean\n",
      "encoder.4.0.shortcut.1.running_var\n",
      "encoder.4.1.conv1.weight\n",
      "encoder.4.1.bn1.weight\n",
      "encoder.4.1.bn1.bias\n",
      "encoder.4.1.bn1.running_mean\n",
      "encoder.4.1.bn1.running_var\n",
      "encoder.4.1.conv2.weight\n",
      "encoder.4.1.bn2.weight\n",
      "encoder.4.1.bn2.bias\n",
      "encoder.4.1.bn2.running_mean\n",
      "encoder.4.1.bn2.running_var\n",
      "encoder.5.0.conv1.weight\n",
      "encoder.5.0.bn1.weight\n",
      "encoder.5.0.bn1.bias\n",
      "encoder.5.0.bn1.running_mean\n",
      "encoder.5.0.bn1.running_var\n",
      "encoder.5.0.conv2.weight\n",
      "encoder.5.0.bn2.weight\n",
      "encoder.5.0.bn2.bias\n",
      "encoder.5.0.bn2.running_mean\n",
      "encoder.5.0.bn2.running_var\n",
      "encoder.5.0.shortcut.0.weight\n",
      "encoder.5.0.shortcut.1.weight\n",
      "encoder.5.0.shortcut.1.bias\n",
      "encoder.5.0.shortcut.1.running_mean\n",
      "encoder.5.0.shortcut.1.running_var\n",
      "encoder.5.1.conv1.weight\n",
      "encoder.5.1.bn1.weight\n",
      "encoder.5.1.bn1.bias\n",
      "encoder.5.1.bn1.running_mean\n",
      "encoder.5.1.bn1.running_var\n",
      "encoder.5.1.conv2.weight\n",
      "encoder.5.1.bn2.weight\n",
      "encoder.5.1.bn2.bias\n",
      "encoder.5.1.bn2.running_mean\n",
      "encoder.5.1.bn2.running_var\n",
      "torch.Size([100])\n",
      "torch.Size([30, 3200])\n",
      "torch.Size([30])\n",
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=3700, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for key, value in mod_weights.items() :\n",
    "    print (key)\n",
    "\n",
    "print(checkpoint['encoder.5.1.bn2.running_var'].shape)\n",
    "print(checkpoint['linear.weight'].shape)\n",
    "print(checkpoint['linear.bias'].shape)\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZrafmBbPpPh"
   },
   "source": [
    "## Making predictions with pre trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pHFLq2MIPrtX"
   },
   "outputs": [],
   "source": [
    "from training import get_predictions\n",
    "from datasets import spectral_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1obozNrShZns",
    "outputId": "95019b09-4179-4859-d2f0-b3cfea40c156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 591 spectra: 18.17s\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on subset of data\n",
    "t0 = time()\n",
    "dl = spectral_dataloader(X, Y, batch_size=10, shuffle=False)\n",
    "y_hat = get_predictions(cnn, dl, cuda)\n",
    "print('Predicted {} spectra: {:0.2f}s'.format(len(y_hat), time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXqUY2cNheIM",
    "outputId": "fa1ddcda-4f37-42b1-beb9-172f3e2188d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.0%\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy\n",
    "acc = (y_hat == Y).mean()\n",
    "print('Accuracy: {:0.1f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e98RRHDBWRnW"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mBKfcq2bWRQ3"
   },
   "outputs": [],
   "source": [
    "from training import run_epoch\n",
    "from torch import optim\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_77FthEEWV7T"
   },
   "source": [
    "### Train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOkvOpR_WY2s",
    "outputId": "35e02987-da0f-4b9d-ac22-2ac1a0064205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\n",
      " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149\n",
      " 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n",
      " 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
      " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
      " 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
      " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311\n",
      " 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\n",
      " 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347\n",
      " 348 349 350 351 352 363 364 365 366 367 368 369 370 371 372 373 374 375\n",
      " 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393\n",
      " 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411\n",
      " 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429\n",
      " 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447\n",
      " 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465\n",
      " 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483\n",
      " 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501\n",
      " 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519\n",
      " 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537\n",
      " 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555\n",
      " 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573\n",
      " 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59 353 354 355 356 357 358 359 360 361 362]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 255 256 257 258 259 260 261 262 263 264 265\n",
      " 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
      " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
      " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
      " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 373 374 375 376 377 378 379 380 381 382 383\n",
      " 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
      " 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 483 531 532 533 534 535 536 537 538\n",
      " 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556\n",
      " 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574\n",
      " 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [241 242 243 244 245 246 247 248 249 250 251 252 253 254 363 364 365 366\n",
      " 367 368 369 370 371 372 484 485 486 487 488 489 490 491 492 493 494 495\n",
      " 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513\n",
      " 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 194 195 196 197 198 199 200 201 202 203 204 205\n",
      " 206 207 208 209 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 335 336 337 338 339 340 341 342 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428\n",
      " 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446\n",
      " 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n",
      " 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n",
      " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536\n",
      " 537 538 539 540 541 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581\n",
      " 582 583 584 585 586 587 588 589 590] TEST: [150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n",
      " 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 343 344 345 346 347 348 349 350 351 352 542 543 544\n",
      " 545 546 547 548 549 550]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 333\n",
      " 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351\n",
      " 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369\n",
      " 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387\n",
      " 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405\n",
      " 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423\n",
      " 424 425 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
      " 554 555 556 557 558 559 560 571 572 573 574 575 576 577 578 579 580 581\n",
      " 582 583 584 585 586 587 588 589 590] TEST: [323 324 325 326 327 328 329 330 331 332 426 427 428 429 430 431 432 433\n",
      " 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451\n",
      " 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469\n",
      " 470 471 472 473 474 475 476 477 478 479 480 481 482 483 561 562 563 564\n",
      " 565 566 567 568 569 570]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 210 211 212 213\n",
      " 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249\n",
      " 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
      " 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285\n",
      " 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
      " 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 383 384 385\n",
      " 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403\n",
      " 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421\n",
      " 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439\n",
      " 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457\n",
      " 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475\n",
      " 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493\n",
      " 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511\n",
      " 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529\n",
      " 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547\n",
      " 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565\n",
      " 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580] TEST: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\n",
      " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149\n",
      " 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 373 374\n",
      " 375 376 377 378 379 380 381 382 581 582 583 584 585 586 587 588 589 590]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 241 242 243 244 245 246 247 248 249\n",
      " 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
      " 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285\n",
      " 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
      " 304 305 306 307 308 309 310 311 312 323 324 325 326 327 328 329 330 331\n",
      " 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349\n",
      " 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [ 78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 313 314\n",
      " 315 316 317 318 319 320 321 322 383 384 385 386 387 388 389 390 391 392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 426 427 428\n",
      " 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446\n",
      " 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n",
      " 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n",
      " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 542 543 544 545 546 547\n",
      " 548 549 550 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [ 60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 531 532 533\n",
      " 534 535 536 537 538 539 540 541 551 552 553 554 555 556 557 558 559 560]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
      " 328 329 330 331 332 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391\n",
      " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
      " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
      " 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445\n",
      " 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463\n",
      " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481\n",
      " 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
      " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 581\n",
      " 582 583 584 585 586 587 588 589 590] TEST: [255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 333 334 335 336 337 338 339 340 341 342 571 572 573 574\n",
      " 575 576 577 578 579 580]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
      " 328 329 330 331 332 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391\n",
      " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
      " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
      " 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445\n",
      " 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463\n",
      " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481\n",
      " 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
      " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 581\n",
      " 582 583 584 585 586 587 588 589 590] [255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 333 334 335 336 337 338 339 340 341 342 571 572 573 574\n",
      " 575 576 577 578 579 580]\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupKFold(n_splits=8)\n",
    "group_kfold.get_n_splits(X_std_pca, Y, groups)\n",
    "for train_index, test_index in group_kfold.split(X, Y, groups):\n",
    "     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train, X_test = X[train_index], X[test_index]\n",
    "     y_train, y_test = Y[train_index], Y[test_index]\n",
    "     #print(X_train, X_test, y_train, y_test)\n",
    "print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwJbWuLsYSev",
    "outputId": "ce50dfb8-9dbd-40ea-e217-6cef69811b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune CNN\n",
    "epochs = 30 # Change this number to ~30 for full training\n",
    "batch_size = 10\n",
    "t0 = time()\n",
    "# Set up Adam optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "# Set up dataloaders\n",
    "dl_tr = spectral_dataloader(X, Y, idxs=train_index,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "dl_val = spectral_dataloader(X, Y, idxs=test_index,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Fine-tune CNN for first fold\n",
    "best_val = 0\n",
    "no_improvement = 0\n",
    "max_no_improvement = 5\n",
    "print('Starting fine-tuning!')\n",
    "for epoch in range(epochs):\n",
    "    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n",
    "    # Train\n",
    "    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n",
    "        training=True, optimizer=optimizer)\n",
    "    print('  Train acc: {:0.2f}'.format(acc_tr))\n",
    "    # Val\n",
    "    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n",
    "        training=False, optimizer=optimizer)\n",
    "    print('  Val acc  : {:0.2f}'.format(acc_val))\n",
    "    # Check performance for early stopping\n",
    "    if acc_val > best_val or epoch == 0:\n",
    "        best_val = acc_val\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    if no_improvement >= max_no_improvement:\n",
    "        print('Finished after {} epochs!'.format(epoch+1))\n",
    "        break\n",
    "\n",
    "print('\\n This demo was completed in: {:0.2f}s'.format(time()-t00))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Raman_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning]",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
