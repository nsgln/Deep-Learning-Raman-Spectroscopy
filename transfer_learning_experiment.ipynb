{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbu3Ct6HA1fy"
   },
   "source": [
    "# Fine-tuning on the reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO1vDaQ4A7MU"
   },
   "source": [
    "Based on the first notebook : https://github.com/csho33/bacteria-ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXO6TrGhBQAu"
   },
   "source": [
    "## Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MEBRu7RX5NwT"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "t00 = time()\n",
    "import numpy as np\n",
    "import os, re, sys\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pd_49FKA_Rl"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yGnfrIMBFaO",
    "outputId": "2e1eb028-87e4-4092-ba2f-40287035e101"
   },
   "outputs": [],
   "source": [
    "#Load from Google Drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#root_dir = \"/content/drive/My Drive/\"\n",
    "#base_dir = root_dir + 'Raman_Data/'\n",
    "#base_dir2 = root_dir + 'Bacteria_TL'\n",
    "\n",
    "#Load from directory\n",
    "os.chdir(os.getcwd())\n",
    "base_dir = 'Raman_Data/'\n",
    "als_dir = base_dir + 'ALS/'\n",
    "ctrl_dir = base_dir + 'CTRL/'\n",
    "\n",
    "base_dir2 = 'Bacteria_TL'\n",
    "sys.path.append(base_dir2)\n",
    "#\n",
    "models = ['pretrained_model.ckpt', 'finetuned_model.ckpt', 'clinical_pretrained_model.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtwEPiqyBo8-",
    "outputId": "fe00e09d-2585-490c-a77b-1324a06d2b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "all_files_als = os.listdir(als_dir)\n",
    "print(len(all_files_als))\n",
    "all_files_ctrl = os.listdir(ctrl_dir)\n",
    "print(len(all_files_ctrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3L4oIupBuVp"
   },
   "source": [
    "Sort files by ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MBFptX6rBxIS"
   },
   "outputs": [],
   "source": [
    "all_files_als.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "all_files_ctrl.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg1xaAkZB27c"
   },
   "source": [
    "### Utility function to parse data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OWMxbzfVB2dJ"
   },
   "outputs": [],
   "source": [
    "def parse_text(file, dir):\n",
    "    with open(dir + file, 'rt') as fd:\n",
    "        data=[]\n",
    "        line = fd.readline()\n",
    "        nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "        data.append(nline)\n",
    "        while line:\n",
    "            line=fd.readline()\n",
    "            nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "            data.append(nline)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkqcDB0eCJzk"
   },
   "source": [
    "### Create X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jvBpmTnnCLjb"
   },
   "outputs": [],
   "source": [
    "X=[] #actual y of spectra\n",
    "Y=[] # 1 -> als; 0 -> ctrl\n",
    "coord=[] #actual x of spectra\n",
    "\n",
    "sep=[60,78,114,150,194,210,225,241,255,280,299,313,323,333,343,353,363,373,383,393] #Il manque le 227\n",
    "groups=[] #for GROUP K FOLD\n",
    "group=0\n",
    "index=1\n",
    "for f in all_files_als:\n",
    "    data=[]\n",
    "    datab=[]\n",
    "    for e in parse_text(f, als_dir):\n",
    "        if len(e) > 0:\n",
    "            datab.append(float(e[0]))\n",
    "            data.append(float(e[1]))\n",
    "    coord.append(datab)\n",
    "    X.append(data)\n",
    "    Y.append(1)\n",
    "    groups.append(group)\n",
    "    if index in sep:\n",
    "        group+=1\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "CZo4eHCBB7qb"
   },
   "outputs": [],
   "source": [
    "#print(len(X))\n",
    "sep=[33,76,91,138,149,158,168,178,188,198]\n",
    "index=1\n",
    "for f in all_files_ctrl:\n",
    "    data=[]\n",
    "    datab=[]\n",
    "    for e in parse_text(f, ctrl_dir):\n",
    "        if len(e) > 0:\n",
    "            datab.append(float(e[0]))\n",
    "            data.append(float(e[1]))\n",
    "    coord.append(datab)\n",
    "    X.append(data)\n",
    "    Y.append(0)\n",
    "    groups.append(group)\n",
    "    if index in sep:\n",
    "        group+=1\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ytc35pqCQbF",
    "outputId": "acfc8500-e701-47b4-bf23-ba1bb4e1aed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 1174) (591,)\n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "groups=np.array(groups)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove negative values from spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range (len(X[i])):\n",
    "        if(X[i][j] < 0):\n",
    "            X[i][j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE-GuahDFSUh"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "UL00DNXfFTzB"
   },
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform\n",
    "X_std = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PX-w-94SFVc-",
    "outputId": "bc242348-d68e-4e6c-9d11-4bacdfacc2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 500)\n"
     ]
    }
   ],
   "source": [
    "# Create a pca object with the 2 components as a parameter\n",
    "pca = decomposition.PCA(n_components=500)\n",
    "\n",
    "# Fit the PCA and transform the data\n",
    "X_std_pca = pca.fit_transform(X_std)\n",
    "print(X_std_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8J3B1hWCb18"
   },
   "source": [
    "## Loading pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "cjBn_Wy1Cevl"
   },
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "layers = 6\n",
    "hidden_size = 100\n",
    "block_size = 2\n",
    "hidden_sizes = [hidden_size] * layers\n",
    "num_blocks = [block_size] * layers\n",
    "input_dim = 1174\n",
    "in_channels = 64\n",
    "n_classes = 2 # 2 classes -> 0 : ctrl & 1 : als\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1QXqHkh9PQWq"
   },
   "outputs": [],
   "source": [
    "#Remove last layers\n",
    "def removekey(d, listofkeys):\n",
    "    r = dict(d)\n",
    "    for key in listofkeys:\n",
    "        print('key: {} is removed'.format(key))\n",
    "        r.pop(key)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MKH-h94Cujy",
    "outputId": "e0c16fd5-a1dd-4ed6-8a00-c01a07af95cf"
   },
   "outputs": [],
   "source": [
    "from resnet import ResNet\n",
    "# Load pre trained model\n",
    "def load_model():\n",
    "    cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n",
    "                    in_channels=in_channels, n_classes=n_classes)\n",
    "    if cuda: cnn.cuda()\n",
    "\n",
    "    ## PROBLEM OF DIFFERING NUMBER OF CLASSES\n",
    "    #cnn.load_state_dict(torch.load('./pretrained_model.ckpt', map_location=lambda storage, loc: storage))\n",
    "\n",
    "    checkpoint = torch.load(base_dir2 + '/' + models[0], map_location=lambda storage, loc: storage)\n",
    "    mod_weights = removekey(checkpoint, ['linear.weight', 'linear.bias'])\n",
    "    cnn.load_state_dict(mod_weights, strict=False)\n",
    "    return cnn, mod_weights, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n"
     ]
    }
   ],
   "source": [
    "cnn, mod_weights, checkpoint = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARAR34kFNWTy",
    "outputId": "d980b2b4-ba07-466a-96fe-e8d08890efe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "encoder.0.0.conv1.weight\n",
      "encoder.0.0.bn1.weight\n",
      "encoder.0.0.bn1.bias\n",
      "encoder.0.0.bn1.running_mean\n",
      "encoder.0.0.bn1.running_var\n",
      "encoder.0.0.conv2.weight\n",
      "encoder.0.0.bn2.weight\n",
      "encoder.0.0.bn2.bias\n",
      "encoder.0.0.bn2.running_mean\n",
      "encoder.0.0.bn2.running_var\n",
      "encoder.0.0.shortcut.0.weight\n",
      "encoder.0.0.shortcut.1.weight\n",
      "encoder.0.0.shortcut.1.bias\n",
      "encoder.0.0.shortcut.1.running_mean\n",
      "encoder.0.0.shortcut.1.running_var\n",
      "encoder.0.1.conv1.weight\n",
      "encoder.0.1.bn1.weight\n",
      "encoder.0.1.bn1.bias\n",
      "encoder.0.1.bn1.running_mean\n",
      "encoder.0.1.bn1.running_var\n",
      "encoder.0.1.conv2.weight\n",
      "encoder.0.1.bn2.weight\n",
      "encoder.0.1.bn2.bias\n",
      "encoder.0.1.bn2.running_mean\n",
      "encoder.0.1.bn2.running_var\n",
      "encoder.1.0.conv1.weight\n",
      "encoder.1.0.bn1.weight\n",
      "encoder.1.0.bn1.bias\n",
      "encoder.1.0.bn1.running_mean\n",
      "encoder.1.0.bn1.running_var\n",
      "encoder.1.0.conv2.weight\n",
      "encoder.1.0.bn2.weight\n",
      "encoder.1.0.bn2.bias\n",
      "encoder.1.0.bn2.running_mean\n",
      "encoder.1.0.bn2.running_var\n",
      "encoder.1.0.shortcut.0.weight\n",
      "encoder.1.0.shortcut.1.weight\n",
      "encoder.1.0.shortcut.1.bias\n",
      "encoder.1.0.shortcut.1.running_mean\n",
      "encoder.1.0.shortcut.1.running_var\n",
      "encoder.1.1.conv1.weight\n",
      "encoder.1.1.bn1.weight\n",
      "encoder.1.1.bn1.bias\n",
      "encoder.1.1.bn1.running_mean\n",
      "encoder.1.1.bn1.running_var\n",
      "encoder.1.1.conv2.weight\n",
      "encoder.1.1.bn2.weight\n",
      "encoder.1.1.bn2.bias\n",
      "encoder.1.1.bn2.running_mean\n",
      "encoder.1.1.bn2.running_var\n",
      "encoder.2.0.conv1.weight\n",
      "encoder.2.0.bn1.weight\n",
      "encoder.2.0.bn1.bias\n",
      "encoder.2.0.bn1.running_mean\n",
      "encoder.2.0.bn1.running_var\n",
      "encoder.2.0.conv2.weight\n",
      "encoder.2.0.bn2.weight\n",
      "encoder.2.0.bn2.bias\n",
      "encoder.2.0.bn2.running_mean\n",
      "encoder.2.0.bn2.running_var\n",
      "encoder.2.0.shortcut.0.weight\n",
      "encoder.2.0.shortcut.1.weight\n",
      "encoder.2.0.shortcut.1.bias\n",
      "encoder.2.0.shortcut.1.running_mean\n",
      "encoder.2.0.shortcut.1.running_var\n",
      "encoder.2.1.conv1.weight\n",
      "encoder.2.1.bn1.weight\n",
      "encoder.2.1.bn1.bias\n",
      "encoder.2.1.bn1.running_mean\n",
      "encoder.2.1.bn1.running_var\n",
      "encoder.2.1.conv2.weight\n",
      "encoder.2.1.bn2.weight\n",
      "encoder.2.1.bn2.bias\n",
      "encoder.2.1.bn2.running_mean\n",
      "encoder.2.1.bn2.running_var\n",
      "encoder.3.0.conv1.weight\n",
      "encoder.3.0.bn1.weight\n",
      "encoder.3.0.bn1.bias\n",
      "encoder.3.0.bn1.running_mean\n",
      "encoder.3.0.bn1.running_var\n",
      "encoder.3.0.conv2.weight\n",
      "encoder.3.0.bn2.weight\n",
      "encoder.3.0.bn2.bias\n",
      "encoder.3.0.bn2.running_mean\n",
      "encoder.3.0.bn2.running_var\n",
      "encoder.3.0.shortcut.0.weight\n",
      "encoder.3.0.shortcut.1.weight\n",
      "encoder.3.0.shortcut.1.bias\n",
      "encoder.3.0.shortcut.1.running_mean\n",
      "encoder.3.0.shortcut.1.running_var\n",
      "encoder.3.1.conv1.weight\n",
      "encoder.3.1.bn1.weight\n",
      "encoder.3.1.bn1.bias\n",
      "encoder.3.1.bn1.running_mean\n",
      "encoder.3.1.bn1.running_var\n",
      "encoder.3.1.conv2.weight\n",
      "encoder.3.1.bn2.weight\n",
      "encoder.3.1.bn2.bias\n",
      "encoder.3.1.bn2.running_mean\n",
      "encoder.3.1.bn2.running_var\n",
      "encoder.4.0.conv1.weight\n",
      "encoder.4.0.bn1.weight\n",
      "encoder.4.0.bn1.bias\n",
      "encoder.4.0.bn1.running_mean\n",
      "encoder.4.0.bn1.running_var\n",
      "encoder.4.0.conv2.weight\n",
      "encoder.4.0.bn2.weight\n",
      "encoder.4.0.bn2.bias\n",
      "encoder.4.0.bn2.running_mean\n",
      "encoder.4.0.bn2.running_var\n",
      "encoder.4.0.shortcut.0.weight\n",
      "encoder.4.0.shortcut.1.weight\n",
      "encoder.4.0.shortcut.1.bias\n",
      "encoder.4.0.shortcut.1.running_mean\n",
      "encoder.4.0.shortcut.1.running_var\n",
      "encoder.4.1.conv1.weight\n",
      "encoder.4.1.bn1.weight\n",
      "encoder.4.1.bn1.bias\n",
      "encoder.4.1.bn1.running_mean\n",
      "encoder.4.1.bn1.running_var\n",
      "encoder.4.1.conv2.weight\n",
      "encoder.4.1.bn2.weight\n",
      "encoder.4.1.bn2.bias\n",
      "encoder.4.1.bn2.running_mean\n",
      "encoder.4.1.bn2.running_var\n",
      "encoder.5.0.conv1.weight\n",
      "encoder.5.0.bn1.weight\n",
      "encoder.5.0.bn1.bias\n",
      "encoder.5.0.bn1.running_mean\n",
      "encoder.5.0.bn1.running_var\n",
      "encoder.5.0.conv2.weight\n",
      "encoder.5.0.bn2.weight\n",
      "encoder.5.0.bn2.bias\n",
      "encoder.5.0.bn2.running_mean\n",
      "encoder.5.0.bn2.running_var\n",
      "encoder.5.0.shortcut.0.weight\n",
      "encoder.5.0.shortcut.1.weight\n",
      "encoder.5.0.shortcut.1.bias\n",
      "encoder.5.0.shortcut.1.running_mean\n",
      "encoder.5.0.shortcut.1.running_var\n",
      "encoder.5.1.conv1.weight\n",
      "encoder.5.1.bn1.weight\n",
      "encoder.5.1.bn1.bias\n",
      "encoder.5.1.bn1.running_mean\n",
      "encoder.5.1.bn1.running_var\n",
      "encoder.5.1.conv2.weight\n",
      "encoder.5.1.bn2.weight\n",
      "encoder.5.1.bn2.bias\n",
      "encoder.5.1.bn2.running_mean\n",
      "encoder.5.1.bn2.running_var\n",
      "torch.Size([100])\n",
      "torch.Size([30, 3200])\n",
      "torch.Size([30])\n",
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=3700, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for key, value in mod_weights.items() :\n",
    "    print (key)\n",
    "\n",
    "print(checkpoint['encoder.5.1.bn2.running_var'].shape)\n",
    "print(checkpoint['linear.weight'].shape)\n",
    "print(checkpoint['linear.bias'].shape)\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZrafmBbPpPh"
   },
   "source": [
    "## Making predictions with pre trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "pHFLq2MIPrtX"
   },
   "outputs": [],
   "source": [
    "from training import get_predictions\n",
    "from datasets import spectral_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1obozNrShZns",
    "outputId": "95019b09-4179-4859-d2f0-b3cfea40c156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 591 spectra: 2.51s\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on subset of data\n",
    "t0 = time()\n",
    "dl = spectral_dataloader(X, Y, batch_size=10, shuffle=False)\n",
    "y_hat = get_predictions(cnn, dl, cuda)\n",
    "print('Predicted {} spectra: {:0.2f}s'.format(len(y_hat), time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXqUY2cNheIM",
    "outputId": "fa1ddcda-4f37-42b1-beb9-172f3e2188d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.4%\n"
     ]
    }
   ],
   "source": [
    "# Computing accuracy\n",
    "acc = (y_hat == Y).mean()\n",
    "print('Accuracy: {:0.1f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e98RRHDBWRnW"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "mBKfcq2bWRQ3"
   },
   "outputs": [],
   "source": [
    "from training import run_epoch\n",
    "from torch import optim\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_77FthEEWV7T"
   },
   "source": [
    "### Train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwJbWuLsYSev",
    "outputId": "ce50dfb8-9dbd-40ea-e217-6cef69811b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 88.48\n",
      "  Val acc  : 98.57\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 87.50\n",
      "  Val acc  : 98.59\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 91.81\n",
      "  Val acc  : 88.46\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 85.96\n",
      "  Val acc  : 88.46\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 85.16\n",
      "  Val acc  : 100.00\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 87.86\n",
      "  Val acc  : 100.00\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 89.79\n",
      "  Val acc  : 95.83\n",
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n",
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n",
      "  Train acc: 89.86\n",
      "  Val acc  : 60.26\n",
      "\n",
      " This demo was completed in: 217.69s\n",
      "91.27197479750296\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "group_kfold.get_n_splits(X, Y, groups)\n",
    "list_accuracy = []\n",
    "for train_index, test_index in group_kfold.split(X, Y, groups):\n",
    "    cnn, _, _ = load_model()\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Fine-tune CNN\n",
    "    epochs = 1 # Change this number to ~30 for full training\n",
    "    batch_size = 10\n",
    "    t0 = time()\n",
    "    # Set up Adam optimizer\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "    # Set up dataloaders\n",
    "    dl_tr = spectral_dataloader(X, Y, idxs=train_index,\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    dl_val = spectral_dataloader(X, Y, idxs=test_index,\n",
    "        batch_size=batch_size, shuffle=False)\n",
    "    # Fine-tune CNN for first fold\n",
    "    best_val = 0\n",
    "    no_improvement = 0\n",
    "    max_no_improvement = 5\n",
    "    print('Starting fine-tuning!')\n",
    "    for epoch in range(epochs):\n",
    "        print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n",
    "        # Train\n",
    "        acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n",
    "            training=True, optimizer=optimizer)\n",
    "        print('  Train acc: {:0.2f}'.format(acc_tr))\n",
    "        # Val\n",
    "        acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n",
    "            training=False, optimizer=optimizer)\n",
    "        print('  Val acc  : {:0.2f}'.format(acc_val))\n",
    "        # Check performance for early stopping\n",
    "        if acc_val > best_val or epoch == 0:\n",
    "            best_val = acc_val\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= max_no_improvement:\n",
    "            print('Finished after {} epochs!'.format(epoch+1))\n",
    "            break\n",
    "\n",
    "    list_accuracy.append(acc_val)\n",
    "    \n",
    "print('\\n This demo was completed in: {:0.2f}s'.format(time()-t00))\n",
    "print(mean(list_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Raman_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
