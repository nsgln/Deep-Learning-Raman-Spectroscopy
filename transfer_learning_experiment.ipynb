{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbu3Ct6HA1fy"
   },
   "source": [
    "# Fine-tuning on the reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO1vDaQ4A7MU"
   },
   "source": [
    "Based on the first notebook : https://github.com/csho33/bacteria-ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXO6TrGhBQAu"
   },
   "source": [
    "## Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MEBRu7RX5NwT"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "t00 = time()\n",
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "import os, re, sys\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pd_49FKA_Rl"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yGnfrIMBFaO",
    "outputId": "2e1eb028-87e4-4092-ba2f-40287035e101"
   },
   "outputs": [],
   "source": [
    "#Load from Google Drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#root_dir = \"/content/drive/My Drive/\"\n",
    "#base_dir = root_dir + 'Raman_Data/'\n",
    "#base_dir2 = root_dir + 'Bacteria_TL'\n",
    "\n",
    "#Load from directory\n",
    "os.chdir(os.getcwd())\n",
    "base_dir = 'Raman_Data/'\n",
    "als_dir = base_dir + 'ALS/'\n",
    "ctrl_dir = base_dir + 'CTRL/'\n",
    "\n",
    "base_dir2 = 'Bacteria_TL'\n",
    "sys.path.append(base_dir2)\n",
    "#\n",
    "models = ['pretrained_model.ckpt', 'finetuned_model.ckpt', 'clinical_pretrained_model.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtwEPiqyBo8-",
    "outputId": "fe00e09d-2585-490c-a77b-1324a06d2b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "all_files_als = os.listdir(als_dir)\n",
    "print(len(all_files_als))\n",
    "all_files_ctrl = os.listdir(ctrl_dir)\n",
    "print(len(all_files_ctrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3L4oIupBuVp"
   },
   "source": [
    "Sort files by ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MBFptX6rBxIS"
   },
   "outputs": [],
   "source": [
    "all_files_als.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "all_files_ctrl.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg1xaAkZB27c"
   },
   "source": [
    "### Utility function to parse data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OWMxbzfVB2dJ"
   },
   "outputs": [],
   "source": [
    "def parse_text(file, dir):\n",
    "  with open(dir + file, 'rt') as fd:\n",
    "    data=[]\n",
    "    line = fd.readline()\n",
    "    nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "    data.append(nline)\n",
    "    while line:\n",
    "      line=fd.readline()\n",
    "      nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "      data.append(nline)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkqcDB0eCJzk"
   },
   "source": [
    "### Create X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jvBpmTnnCLjb"
   },
   "outputs": [],
   "source": [
    "X=[] #actual y of spectra\n",
    "Y=[] # 1 -> als; 0 -> ctrl\n",
    "coord=[] #actual x of spectra\n",
    "\n",
    "sep=[60,78,114,150,194,210,225,241,255,280,299,313,323,333,343,353,363,373,383,393] #Il manque le 227\n",
    "groups=[] #for GROUP K FOLD\n",
    "group=0\n",
    "index=1\n",
    "for f in all_files_als:\n",
    "  data=[]\n",
    "  datab=[]\n",
    "  for e in parse_text(f, als_dir):\n",
    "    if len(e) > 0:\n",
    "      datab.append(float(e[0]))\n",
    "      data.append(float(e[1]))\n",
    "  coord.append(datab)\n",
    "  X.append(data)\n",
    "  Y.append(1)\n",
    "  groups.append(group)\n",
    "  if index in sep:\n",
    "    group+=1\n",
    "  index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CZo4eHCBB7qb"
   },
   "outputs": [],
   "source": [
    "#print(len(X))\n",
    "sep=[33,76,91,138,149,158,168,178,188,198]\n",
    "index=1\n",
    "for f in all_files_ctrl:\n",
    "  data=[]\n",
    "  datab=[]\n",
    "  for e in parse_text(f, ctrl_dir):\n",
    "    if len(e) > 0:\n",
    "      datab.append(float(e[0]))\n",
    "      data.append(float(e[1]))\n",
    "  coord.append(datab)\n",
    "  X.append(data)\n",
    "  Y.append(0)\n",
    "  groups.append(group)\n",
    "  if index in sep:\n",
    "    group+=1\n",
    "  index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ytc35pqCQbF",
    "outputId": "acfc8500-e701-47b4-bf23-ba1bb4e1aed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 1174) (591,)\n"
     ]
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "groups=np.array(groups)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE-GuahDFSUh"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UL00DNXfFTzB"
   },
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform\n",
    "X_std = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PX-w-94SFVc-",
    "outputId": "bc242348-d68e-4e6c-9d11-4bacdfacc2bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 500)\n"
     ]
    }
   ],
   "source": [
    "# Create a pca object with the 2 components as a parameter\n",
    "pca = decomposition.PCA(n_components=500)\n",
    "\n",
    "# Fit the PCA and transform the data\n",
    "X_std_pca = pca.fit_transform(X_std)\n",
    "print(X_std_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8J3B1hWCb18"
   },
   "source": [
    "## Loading pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cjBn_Wy1Cevl"
   },
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "layers = 6\n",
    "hidden_size = 100\n",
    "block_size = 2\n",
    "hidden_sizes = [hidden_size] * layers\n",
    "num_blocks = [block_size] * layers\n",
    "input_dim = 1174\n",
    "in_channels = 64\n",
    "n_classes = 2 # 2 classes -> 0 : ctrl & 1 : als\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1QXqHkh9PQWq"
   },
   "outputs": [],
   "source": [
    "def removekey(d, listofkeys):\n",
    "    r = dict(d)\n",
    "    for key in listofkeys:\n",
    "        print('key: {} is removed'.format(key))\n",
    "        r.pop(key)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MKH-h94Cujy",
    "outputId": "e0c16fd5-a1dd-4ed6-8a00-c01a07af95cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: linear.weight is removed\n",
      "key: linear.bias is removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['linear.weight', 'linear.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from resnet import ResNet\n",
    "# Load trained weights for demo\n",
    "cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n",
    "                in_channels=in_channels, n_classes=n_classes)\n",
    "if cuda: cnn.cuda()\n",
    "\n",
    "## PROBLEM OF DIFFERING NUMBER OF CLASSES\n",
    "#cnn.load_state_dict(torch.load('./pretrained_model.ckpt', map_location=lambda storage, loc: storage))\n",
    "\n",
    "checkpoint = torch.load(base_dir2 + '/' + models[0], map_location=lambda storage, loc: storage)\n",
    "mod_weights = removekey(checkpoint, ['linear.weight', 'linear.bias'])\n",
    "cnn.load_state_dict(mod_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARAR34kFNWTy",
    "outputId": "d980b2b4-ba07-466a-96fe-e8d08890efe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "encoder.0.0.conv1.weight\n",
      "encoder.0.0.bn1.weight\n",
      "encoder.0.0.bn1.bias\n",
      "encoder.0.0.bn1.running_mean\n",
      "encoder.0.0.bn1.running_var\n",
      "encoder.0.0.conv2.weight\n",
      "encoder.0.0.bn2.weight\n",
      "encoder.0.0.bn2.bias\n",
      "encoder.0.0.bn2.running_mean\n",
      "encoder.0.0.bn2.running_var\n",
      "encoder.0.0.shortcut.0.weight\n",
      "encoder.0.0.shortcut.1.weight\n",
      "encoder.0.0.shortcut.1.bias\n",
      "encoder.0.0.shortcut.1.running_mean\n",
      "encoder.0.0.shortcut.1.running_var\n",
      "encoder.0.1.conv1.weight\n",
      "encoder.0.1.bn1.weight\n",
      "encoder.0.1.bn1.bias\n",
      "encoder.0.1.bn1.running_mean\n",
      "encoder.0.1.bn1.running_var\n",
      "encoder.0.1.conv2.weight\n",
      "encoder.0.1.bn2.weight\n",
      "encoder.0.1.bn2.bias\n",
      "encoder.0.1.bn2.running_mean\n",
      "encoder.0.1.bn2.running_var\n",
      "encoder.1.0.conv1.weight\n",
      "encoder.1.0.bn1.weight\n",
      "encoder.1.0.bn1.bias\n",
      "encoder.1.0.bn1.running_mean\n",
      "encoder.1.0.bn1.running_var\n",
      "encoder.1.0.conv2.weight\n",
      "encoder.1.0.bn2.weight\n",
      "encoder.1.0.bn2.bias\n",
      "encoder.1.0.bn2.running_mean\n",
      "encoder.1.0.bn2.running_var\n",
      "encoder.1.0.shortcut.0.weight\n",
      "encoder.1.0.shortcut.1.weight\n",
      "encoder.1.0.shortcut.1.bias\n",
      "encoder.1.0.shortcut.1.running_mean\n",
      "encoder.1.0.shortcut.1.running_var\n",
      "encoder.1.1.conv1.weight\n",
      "encoder.1.1.bn1.weight\n",
      "encoder.1.1.bn1.bias\n",
      "encoder.1.1.bn1.running_mean\n",
      "encoder.1.1.bn1.running_var\n",
      "encoder.1.1.conv2.weight\n",
      "encoder.1.1.bn2.weight\n",
      "encoder.1.1.bn2.bias\n",
      "encoder.1.1.bn2.running_mean\n",
      "encoder.1.1.bn2.running_var\n",
      "encoder.2.0.conv1.weight\n",
      "encoder.2.0.bn1.weight\n",
      "encoder.2.0.bn1.bias\n",
      "encoder.2.0.bn1.running_mean\n",
      "encoder.2.0.bn1.running_var\n",
      "encoder.2.0.conv2.weight\n",
      "encoder.2.0.bn2.weight\n",
      "encoder.2.0.bn2.bias\n",
      "encoder.2.0.bn2.running_mean\n",
      "encoder.2.0.bn2.running_var\n",
      "encoder.2.0.shortcut.0.weight\n",
      "encoder.2.0.shortcut.1.weight\n",
      "encoder.2.0.shortcut.1.bias\n",
      "encoder.2.0.shortcut.1.running_mean\n",
      "encoder.2.0.shortcut.1.running_var\n",
      "encoder.2.1.conv1.weight\n",
      "encoder.2.1.bn1.weight\n",
      "encoder.2.1.bn1.bias\n",
      "encoder.2.1.bn1.running_mean\n",
      "encoder.2.1.bn1.running_var\n",
      "encoder.2.1.conv2.weight\n",
      "encoder.2.1.bn2.weight\n",
      "encoder.2.1.bn2.bias\n",
      "encoder.2.1.bn2.running_mean\n",
      "encoder.2.1.bn2.running_var\n",
      "encoder.3.0.conv1.weight\n",
      "encoder.3.0.bn1.weight\n",
      "encoder.3.0.bn1.bias\n",
      "encoder.3.0.bn1.running_mean\n",
      "encoder.3.0.bn1.running_var\n",
      "encoder.3.0.conv2.weight\n",
      "encoder.3.0.bn2.weight\n",
      "encoder.3.0.bn2.bias\n",
      "encoder.3.0.bn2.running_mean\n",
      "encoder.3.0.bn2.running_var\n",
      "encoder.3.0.shortcut.0.weight\n",
      "encoder.3.0.shortcut.1.weight\n",
      "encoder.3.0.shortcut.1.bias\n",
      "encoder.3.0.shortcut.1.running_mean\n",
      "encoder.3.0.shortcut.1.running_var\n",
      "encoder.3.1.conv1.weight\n",
      "encoder.3.1.bn1.weight\n",
      "encoder.3.1.bn1.bias\n",
      "encoder.3.1.bn1.running_mean\n",
      "encoder.3.1.bn1.running_var\n",
      "encoder.3.1.conv2.weight\n",
      "encoder.3.1.bn2.weight\n",
      "encoder.3.1.bn2.bias\n",
      "encoder.3.1.bn2.running_mean\n",
      "encoder.3.1.bn2.running_var\n",
      "encoder.4.0.conv1.weight\n",
      "encoder.4.0.bn1.weight\n",
      "encoder.4.0.bn1.bias\n",
      "encoder.4.0.bn1.running_mean\n",
      "encoder.4.0.bn1.running_var\n",
      "encoder.4.0.conv2.weight\n",
      "encoder.4.0.bn2.weight\n",
      "encoder.4.0.bn2.bias\n",
      "encoder.4.0.bn2.running_mean\n",
      "encoder.4.0.bn2.running_var\n",
      "encoder.4.0.shortcut.0.weight\n",
      "encoder.4.0.shortcut.1.weight\n",
      "encoder.4.0.shortcut.1.bias\n",
      "encoder.4.0.shortcut.1.running_mean\n",
      "encoder.4.0.shortcut.1.running_var\n",
      "encoder.4.1.conv1.weight\n",
      "encoder.4.1.bn1.weight\n",
      "encoder.4.1.bn1.bias\n",
      "encoder.4.1.bn1.running_mean\n",
      "encoder.4.1.bn1.running_var\n",
      "encoder.4.1.conv2.weight\n",
      "encoder.4.1.bn2.weight\n",
      "encoder.4.1.bn2.bias\n",
      "encoder.4.1.bn2.running_mean\n",
      "encoder.4.1.bn2.running_var\n",
      "encoder.5.0.conv1.weight\n",
      "encoder.5.0.bn1.weight\n",
      "encoder.5.0.bn1.bias\n",
      "encoder.5.0.bn1.running_mean\n",
      "encoder.5.0.bn1.running_var\n",
      "encoder.5.0.conv2.weight\n",
      "encoder.5.0.bn2.weight\n",
      "encoder.5.0.bn2.bias\n",
      "encoder.5.0.bn2.running_mean\n",
      "encoder.5.0.bn2.running_var\n",
      "encoder.5.0.shortcut.0.weight\n",
      "encoder.5.0.shortcut.1.weight\n",
      "encoder.5.0.shortcut.1.bias\n",
      "encoder.5.0.shortcut.1.running_mean\n",
      "encoder.5.0.shortcut.1.running_var\n",
      "encoder.5.1.conv1.weight\n",
      "encoder.5.1.bn1.weight\n",
      "encoder.5.1.bn1.bias\n",
      "encoder.5.1.bn1.running_mean\n",
      "encoder.5.1.bn1.running_var\n",
      "encoder.5.1.conv2.weight\n",
      "encoder.5.1.bn2.weight\n",
      "encoder.5.1.bn2.bias\n",
      "encoder.5.1.bn2.running_mean\n",
      "encoder.5.1.bn2.running_var\n",
      "torch.Size([100])\n",
      "torch.Size([30, 3200])\n",
      "torch.Size([30])\n",
      "ResNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
      "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=3700, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for key, value in mod_weights.items() :\n",
    "    print (key)\n",
    "\n",
    "print(checkpoint['encoder.5.1.bn2.running_var'].shape)\n",
    "print(checkpoint['linear.weight'].shape)\n",
    "print(checkpoint['linear.bias'].shape)\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZrafmBbPpPh"
   },
   "source": [
    "## Making predictions with pre trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pHFLq2MIPrtX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8110cda566e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspectral_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\Advanced_Machine_Learning\\Projet\\Deep-Learning-Raman-Spectroscopy\\Bacteria_TL\\datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from training import get_predictions\n",
    "from datasets import spectral_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1obozNrShZns",
    "outputId": "95019b09-4179-4859-d2f0-b3cfea40c156"
   },
   "outputs": [],
   "source": [
    "# Make predictions on subset of data\n",
    "t0 = time()\n",
    "dl = spectral_dataloader(X, Y, batch_size=10, shuffle=False)\n",
    "y_hat = get_predictions(cnn, dl, cuda)\n",
    "print('Predicted {} spectra: {:0.2f}s'.format(len(y_hat), time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXqUY2cNheIM",
    "outputId": "fa1ddcda-4f37-42b1-beb9-172f3e2188d3"
   },
   "outputs": [],
   "source": [
    "# Computing accuracy\n",
    "acc = (y_hat == Y).mean()\n",
    "print('Accuracy: {:0.1f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e98RRHDBWRnW"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBKfcq2bWRQ3"
   },
   "outputs": [],
   "source": [
    "from training import run_epoch\n",
    "from torch import optim\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_77FthEEWV7T"
   },
   "source": [
    "### Train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOkvOpR_WY2s",
    "outputId": "35e02987-da0f-4b9d-ac22-2ac1a0064205"
   },
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=8)\n",
    "group_kfold.get_n_splits(X_std_pca, Y, groups)\n",
    "for train_index, test_index in group_kfold.split(X, Y, groups):\n",
    "     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train, X_test = X[train_index], X[test_index]\n",
    "     y_train, y_test = Y[train_index], Y[test_index]\n",
    "     #print(X_train, X_test, y_train, y_test)\n",
    "print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwJbWuLsYSev",
    "outputId": "ce50dfb8-9dbd-40ea-e217-6cef69811b6d"
   },
   "outputs": [],
   "source": [
    "# Fine-tune CNN\n",
    "epochs = 30 # Change this number to ~30 for full training\n",
    "batch_size = 10\n",
    "t0 = time()\n",
    "# Set up Adam optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "# Set up dataloaders\n",
    "dl_tr = spectral_dataloader(X, Y, idxs=train_index,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "dl_val = spectral_dataloader(X, Y, idxs=test_index,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Fine-tune CNN for first fold\n",
    "best_val = 0\n",
    "no_improvement = 0\n",
    "max_no_improvement = 5\n",
    "print('Starting fine-tuning!')\n",
    "for epoch in range(epochs):\n",
    "    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n",
    "    # Train\n",
    "    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n",
    "        training=True, optimizer=optimizer)\n",
    "    print('  Train acc: {:0.2f}'.format(acc_tr))\n",
    "    # Val\n",
    "    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n",
    "        training=False, optimizer=optimizer)\n",
    "    print('  Val acc  : {:0.2f}'.format(acc_val))\n",
    "    # Check performance for early stopping\n",
    "    if acc_val > best_val or epoch == 0:\n",
    "        best_val = acc_val\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    if no_improvement >= max_no_improvement:\n",
    "        print('Finished after {} epochs!'.format(epoch+1))\n",
    "        break\n",
    "\n",
    "print('\\n This demo was completed in: {:0.2f}s'.format(time()-t00))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Raman_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning]",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
