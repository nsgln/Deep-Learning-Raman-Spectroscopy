{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Raman_TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbu3Ct6HA1fy"
      },
      "source": [
        "# Fine-tuning on the reference dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO1vDaQ4A7MU"
      },
      "source": [
        "Based on the first notebook : https://github.com/csho33/bacteria-ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXO6TrGhBQAu"
      },
      "source": [
        "## Libraries and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEBRu7RX5NwT"
      },
      "source": [
        "from time import time\r\n",
        "t00 = time()\r\n",
        "import numpy as np\r\n",
        "from google.colab import drive\r\n",
        "import os, re, sys\r\n",
        "import tensorflow as tf\r\n",
        "import torch\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn import decomposition"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pd_49FKA_Rl"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yGnfrIMBFaO",
        "outputId": "2e1eb028-87e4-4092-ba2f-40287035e101"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)\r\n",
        "root_dir = \"/content/drive/My Drive/\"\r\n",
        "base_dir = root_dir + 'Raman_Data/'\r\n",
        "als_dir = base_dir + 'ALS/'\r\n",
        "ctrl_dir = base_dir + 'CTRL/'\r\n",
        "\r\n",
        "base_dir2 = root_dir + 'Bacteria_TL'\r\n",
        "sys.path.append(base_dir2)\r\n",
        "\r\n",
        "models = ['pretrained_model.ckpt', 'finetuned_model.ckpt', 'clinical_pretrained_model.ckpt']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtwEPiqyBo8-",
        "outputId": "fe00e09d-2585-490c-a77b-1324a06d2b43"
      },
      "source": [
        "all_files_als = os.listdir(als_dir)\r\n",
        "print(len(all_files_als))\r\n",
        "all_files_ctrl = os.listdir(ctrl_dir)\r\n",
        "print(len(all_files_ctrl))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "393\n",
            "198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3L4oIupBuVp"
      },
      "source": [
        "Sort files by ascending order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBFptX6rBxIS"
      },
      "source": [
        "all_files_als.sort(key=lambda f: int(re.sub('\\D', '', f)))\r\n",
        "all_files_ctrl.sort(key=lambda f: int(re.sub('\\D', '', f)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg1xaAkZB27c"
      },
      "source": [
        "### Utility function to parse data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWMxbzfVB2dJ"
      },
      "source": [
        "def parse_text(file, dir):\r\n",
        "  with open(dir + file, 'rt') as fd:\r\n",
        "    data=[]\r\n",
        "    line = fd.readline()\r\n",
        "    nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\r\n",
        "    data.append(nline)\r\n",
        "    while line:\r\n",
        "      line=fd.readline()\r\n",
        "      nline = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\r\n",
        "      data.append(nline)\r\n",
        "  return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkqcDB0eCJzk"
      },
      "source": [
        "### Create X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvBpmTnnCLjb"
      },
      "source": [
        "X=[] #actual y of spectra\r\n",
        "Y=[] # 1 -> als; 0 -> ctrl\r\n",
        "coord=[] #actual x of spectra\r\n",
        "\r\n",
        "sep=[60,78,114,150,194,210,225,241,255,280,299,313,323,333,343,353,363,373,383,393] #Il manque le 227\r\n",
        "groups=[] #for GROUP K FOLD\r\n",
        "group=0\r\n",
        "index=1\r\n",
        "for f in all_files_als:\r\n",
        "  data=[]\r\n",
        "  datab=[]\r\n",
        "  for e in parse_text(f, als_dir):\r\n",
        "    if len(e) > 0:\r\n",
        "      datab.append(float(e[0]))\r\n",
        "      data.append(float(e[1]))\r\n",
        "  coord.append(datab)\r\n",
        "  X.append(data)\r\n",
        "  Y.append(1)\r\n",
        "  groups.append(group)\r\n",
        "  if index in sep:\r\n",
        "    group+=1\r\n",
        "  index+=1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZo4eHCBB7qb"
      },
      "source": [
        "#print(len(X))\r\n",
        "sep=[33,76,91,138,149,158,168,178,188,198]\r\n",
        "index=1\r\n",
        "for f in all_files_ctrl:\r\n",
        "  data=[]\r\n",
        "  datab=[]\r\n",
        "  for e in parse_text(f, ctrl_dir):\r\n",
        "    if len(e) > 0:\r\n",
        "      datab.append(float(e[0]))\r\n",
        "      data.append(float(e[1]))\r\n",
        "  coord.append(datab)\r\n",
        "  X.append(data)\r\n",
        "  Y.append(0)\r\n",
        "  groups.append(group)\r\n",
        "  if index in sep:\r\n",
        "    group+=1\r\n",
        "  index+=1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ytc35pqCQbF",
        "outputId": "acfc8500-e701-47b4-bf23-ba1bb4e1aed2"
      },
      "source": [
        "X=np.array(X)\r\n",
        "Y=np.array(Y)\r\n",
        "groups=np.array(groups)\r\n",
        "print(X.shape, Y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(591, 1174) (591,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE-GuahDFSUh"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL00DNXfFTzB"
      },
      "source": [
        "# Create a scaler object\r\n",
        "sc = StandardScaler()\r\n",
        "\r\n",
        "# Fit the scaler to the features and transform\r\n",
        "X_std = sc.fit_transform(X)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX-w-94SFVc-",
        "outputId": "bc242348-d68e-4e6c-9d11-4bacdfacc2bc"
      },
      "source": [
        "# Create a pca object with the 2 components as a parameter\r\n",
        "pca = decomposition.PCA(n_components=500)\r\n",
        "\r\n",
        "# Fit the PCA and transform the data\r\n",
        "X_std_pca = pca.fit_transform(X_std)\r\n",
        "print(X_std_pca.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(591, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8J3B1hWCb18"
      },
      "source": [
        "## Loading pre-trained CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjBn_Wy1Cevl"
      },
      "source": [
        "# CNN parameters\r\n",
        "layers = 6\r\n",
        "hidden_size = 100\r\n",
        "block_size = 2\r\n",
        "hidden_sizes = [hidden_size] * layers\r\n",
        "num_blocks = [block_size] * layers\r\n",
        "input_dim = 1174\r\n",
        "in_channels = 64\r\n",
        "n_classes = 2 # 2 classes -> 0 : ctrl & 1 : als\r\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\r\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QXqHkh9PQWq"
      },
      "source": [
        "def removekey(d, listofkeys):\r\n",
        "    r = dict(d)\r\n",
        "    for key in listofkeys:\r\n",
        "        print('key: {} is removed'.format(key))\r\n",
        "        r.pop(key)\r\n",
        "    return r"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MKH-h94Cujy",
        "outputId": "e0c16fd5-a1dd-4ed6-8a00-c01a07af95cf"
      },
      "source": [
        "from resnet import ResNet\r\n",
        "# Load trained weights for demo\r\n",
        "cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\r\n",
        "                in_channels=in_channels, n_classes=n_classes)\r\n",
        "if cuda: cnn.cuda()\r\n",
        "\r\n",
        "## PROBLEM OF DIFFERING NUMBER OF CLASSES\r\n",
        "#cnn.load_state_dict(torch.load('./pretrained_model.ckpt', map_location=lambda storage, loc: storage))\r\n",
        "\r\n",
        "checkpoint = torch.load(base_dir2 + '/' + models[0], map_location=lambda storage, loc: storage)\r\n",
        "mod_weights = removekey(checkpoint, ['linear.weight', 'linear.bias'])\r\n",
        "cnn.load_state_dict(mod_weights, strict=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key: linear.weight is removed\n",
            "key: linear.bias is removed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['linear.weight', 'linear.bias'], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARAR34kFNWTy",
        "outputId": "d980b2b4-ba07-466a-96fe-e8d08890efe9"
      },
      "source": [
        "for key, value in mod_weights.items() :\r\n",
        "    print (key)\r\n",
        "\r\n",
        "print(checkpoint['encoder.5.1.bn2.running_var'].shape)\r\n",
        "print(checkpoint['linear.weight'].shape)\r\n",
        "print(checkpoint['linear.bias'].shape)\r\n",
        "\r\n",
        "print(cnn)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.weight\n",
            "bn1.weight\n",
            "bn1.bias\n",
            "bn1.running_mean\n",
            "bn1.running_var\n",
            "encoder.0.0.conv1.weight\n",
            "encoder.0.0.bn1.weight\n",
            "encoder.0.0.bn1.bias\n",
            "encoder.0.0.bn1.running_mean\n",
            "encoder.0.0.bn1.running_var\n",
            "encoder.0.0.conv2.weight\n",
            "encoder.0.0.bn2.weight\n",
            "encoder.0.0.bn2.bias\n",
            "encoder.0.0.bn2.running_mean\n",
            "encoder.0.0.bn2.running_var\n",
            "encoder.0.0.shortcut.0.weight\n",
            "encoder.0.0.shortcut.1.weight\n",
            "encoder.0.0.shortcut.1.bias\n",
            "encoder.0.0.shortcut.1.running_mean\n",
            "encoder.0.0.shortcut.1.running_var\n",
            "encoder.0.1.conv1.weight\n",
            "encoder.0.1.bn1.weight\n",
            "encoder.0.1.bn1.bias\n",
            "encoder.0.1.bn1.running_mean\n",
            "encoder.0.1.bn1.running_var\n",
            "encoder.0.1.conv2.weight\n",
            "encoder.0.1.bn2.weight\n",
            "encoder.0.1.bn2.bias\n",
            "encoder.0.1.bn2.running_mean\n",
            "encoder.0.1.bn2.running_var\n",
            "encoder.1.0.conv1.weight\n",
            "encoder.1.0.bn1.weight\n",
            "encoder.1.0.bn1.bias\n",
            "encoder.1.0.bn1.running_mean\n",
            "encoder.1.0.bn1.running_var\n",
            "encoder.1.0.conv2.weight\n",
            "encoder.1.0.bn2.weight\n",
            "encoder.1.0.bn2.bias\n",
            "encoder.1.0.bn2.running_mean\n",
            "encoder.1.0.bn2.running_var\n",
            "encoder.1.0.shortcut.0.weight\n",
            "encoder.1.0.shortcut.1.weight\n",
            "encoder.1.0.shortcut.1.bias\n",
            "encoder.1.0.shortcut.1.running_mean\n",
            "encoder.1.0.shortcut.1.running_var\n",
            "encoder.1.1.conv1.weight\n",
            "encoder.1.1.bn1.weight\n",
            "encoder.1.1.bn1.bias\n",
            "encoder.1.1.bn1.running_mean\n",
            "encoder.1.1.bn1.running_var\n",
            "encoder.1.1.conv2.weight\n",
            "encoder.1.1.bn2.weight\n",
            "encoder.1.1.bn2.bias\n",
            "encoder.1.1.bn2.running_mean\n",
            "encoder.1.1.bn2.running_var\n",
            "encoder.2.0.conv1.weight\n",
            "encoder.2.0.bn1.weight\n",
            "encoder.2.0.bn1.bias\n",
            "encoder.2.0.bn1.running_mean\n",
            "encoder.2.0.bn1.running_var\n",
            "encoder.2.0.conv2.weight\n",
            "encoder.2.0.bn2.weight\n",
            "encoder.2.0.bn2.bias\n",
            "encoder.2.0.bn2.running_mean\n",
            "encoder.2.0.bn2.running_var\n",
            "encoder.2.0.shortcut.0.weight\n",
            "encoder.2.0.shortcut.1.weight\n",
            "encoder.2.0.shortcut.1.bias\n",
            "encoder.2.0.shortcut.1.running_mean\n",
            "encoder.2.0.shortcut.1.running_var\n",
            "encoder.2.1.conv1.weight\n",
            "encoder.2.1.bn1.weight\n",
            "encoder.2.1.bn1.bias\n",
            "encoder.2.1.bn1.running_mean\n",
            "encoder.2.1.bn1.running_var\n",
            "encoder.2.1.conv2.weight\n",
            "encoder.2.1.bn2.weight\n",
            "encoder.2.1.bn2.bias\n",
            "encoder.2.1.bn2.running_mean\n",
            "encoder.2.1.bn2.running_var\n",
            "encoder.3.0.conv1.weight\n",
            "encoder.3.0.bn1.weight\n",
            "encoder.3.0.bn1.bias\n",
            "encoder.3.0.bn1.running_mean\n",
            "encoder.3.0.bn1.running_var\n",
            "encoder.3.0.conv2.weight\n",
            "encoder.3.0.bn2.weight\n",
            "encoder.3.0.bn2.bias\n",
            "encoder.3.0.bn2.running_mean\n",
            "encoder.3.0.bn2.running_var\n",
            "encoder.3.0.shortcut.0.weight\n",
            "encoder.3.0.shortcut.1.weight\n",
            "encoder.3.0.shortcut.1.bias\n",
            "encoder.3.0.shortcut.1.running_mean\n",
            "encoder.3.0.shortcut.1.running_var\n",
            "encoder.3.1.conv1.weight\n",
            "encoder.3.1.bn1.weight\n",
            "encoder.3.1.bn1.bias\n",
            "encoder.3.1.bn1.running_mean\n",
            "encoder.3.1.bn1.running_var\n",
            "encoder.3.1.conv2.weight\n",
            "encoder.3.1.bn2.weight\n",
            "encoder.3.1.bn2.bias\n",
            "encoder.3.1.bn2.running_mean\n",
            "encoder.3.1.bn2.running_var\n",
            "encoder.4.0.conv1.weight\n",
            "encoder.4.0.bn1.weight\n",
            "encoder.4.0.bn1.bias\n",
            "encoder.4.0.bn1.running_mean\n",
            "encoder.4.0.bn1.running_var\n",
            "encoder.4.0.conv2.weight\n",
            "encoder.4.0.bn2.weight\n",
            "encoder.4.0.bn2.bias\n",
            "encoder.4.0.bn2.running_mean\n",
            "encoder.4.0.bn2.running_var\n",
            "encoder.4.0.shortcut.0.weight\n",
            "encoder.4.0.shortcut.1.weight\n",
            "encoder.4.0.shortcut.1.bias\n",
            "encoder.4.0.shortcut.1.running_mean\n",
            "encoder.4.0.shortcut.1.running_var\n",
            "encoder.4.1.conv1.weight\n",
            "encoder.4.1.bn1.weight\n",
            "encoder.4.1.bn1.bias\n",
            "encoder.4.1.bn1.running_mean\n",
            "encoder.4.1.bn1.running_var\n",
            "encoder.4.1.conv2.weight\n",
            "encoder.4.1.bn2.weight\n",
            "encoder.4.1.bn2.bias\n",
            "encoder.4.1.bn2.running_mean\n",
            "encoder.4.1.bn2.running_var\n",
            "encoder.5.0.conv1.weight\n",
            "encoder.5.0.bn1.weight\n",
            "encoder.5.0.bn1.bias\n",
            "encoder.5.0.bn1.running_mean\n",
            "encoder.5.0.bn1.running_var\n",
            "encoder.5.0.conv2.weight\n",
            "encoder.5.0.bn2.weight\n",
            "encoder.5.0.bn2.bias\n",
            "encoder.5.0.bn2.running_mean\n",
            "encoder.5.0.bn2.running_var\n",
            "encoder.5.0.shortcut.0.weight\n",
            "encoder.5.0.shortcut.1.weight\n",
            "encoder.5.0.shortcut.1.bias\n",
            "encoder.5.0.shortcut.1.running_mean\n",
            "encoder.5.0.shortcut.1.running_var\n",
            "encoder.5.1.conv1.weight\n",
            "encoder.5.1.bn1.weight\n",
            "encoder.5.1.bn1.bias\n",
            "encoder.5.1.bn1.running_mean\n",
            "encoder.5.1.bn1.running_var\n",
            "encoder.5.1.conv2.weight\n",
            "encoder.5.1.bn2.weight\n",
            "encoder.5.1.bn2.bias\n",
            "encoder.5.1.bn2.running_mean\n",
            "encoder.5.1.bn2.running_var\n",
            "torch.Size([100])\n",
            "torch.Size([30, 3200])\n",
            "torch.Size([30])\n",
            "ResNet(\n",
            "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (encoder): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
            "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
            "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
            "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
            "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n",
            "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=3700, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZrafmBbPpPh"
      },
      "source": [
        "## Making predictions with pre trained CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHFLq2MIPrtX"
      },
      "source": [
        "from training import get_predictions\r\n",
        "from datasets import spectral_dataloader"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1obozNrShZns",
        "outputId": "95019b09-4179-4859-d2f0-b3cfea40c156"
      },
      "source": [
        "# Make predictions on subset of data\r\n",
        "t0 = time()\r\n",
        "dl = spectral_dataloader(X, Y, batch_size=10, shuffle=False)\r\n",
        "y_hat = get_predictions(cnn, dl, cuda)\r\n",
        "print('Predicted {} spectra: {:0.2f}s'.format(len(y_hat), time()-t0))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted 591 spectra: 0.81s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXqUY2cNheIM",
        "outputId": "fa1ddcda-4f37-42b1-beb9-172f3e2188d3"
      },
      "source": [
        "# Computing accuracy\r\n",
        "acc = (y_hat == Y).mean()\r\n",
        "print('Accuracy: {:0.1f}%'.format(100*acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 69.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e98RRHDBWRnW"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBKfcq2bWRQ3"
      },
      "source": [
        "from training import run_epoch\r\n",
        "from torch import optim\r\n",
        "from sklearn.model_selection import GroupKFold"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77FthEEWV7T"
      },
      "source": [
        "### Train/val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOkvOpR_WY2s",
        "outputId": "35e02987-da0f-4b9d-ac22-2ac1a0064205"
      },
      "source": [
        "group_kfold = GroupKFold(n_splits=8)\r\n",
        "group_kfold.get_n_splits(X_std_pca, Y, groups)\r\n",
        "for train_index, test_index in group_kfold.split(X, Y, groups):\r\n",
        "     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\r\n",
        "     X_train, X_test = X[train_index], X[test_index]\r\n",
        "     y_train, y_test = Y[train_index], Y[test_index]\r\n",
        "     #print(X_train, X_test, y_train, y_test)\r\n",
        "print(train_index, test_index)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [ 60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
            "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
            " 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\n",
            " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149\n",
            " 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n",
            " 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
            " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
            " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
            " 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
            " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
            " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
            " 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311\n",
            " 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\n",
            " 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347\n",
            " 348 349 350 351 352 363 364 365 366 367 368 369 370 371 372 373 374 375\n",
            " 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393\n",
            " 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411\n",
            " 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429\n",
            " 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447\n",
            " 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465\n",
            " 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483\n",
            " 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501\n",
            " 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519\n",
            " 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537\n",
            " 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555\n",
            " 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573\n",
            " 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59 353 354 355 356 357 358 359 360 361 362]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 255 256 257 258 259 260 261 262 263 264 265\n",
            " 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
            " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
            " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
            " 356 357 358 359 360 361 362 373 374 375 376 377 378 379 380 381 382 383\n",
            " 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
            " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
            " 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
            " 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
            " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
            " 474 475 476 477 478 479 480 481 482 483 531 532 533 534 535 536 537 538\n",
            " 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556\n",
            " 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574\n",
            " 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [241 242 243 244 245 246 247 248 249 250 251 252 253 254 363 364 365 366\n",
            " 367 368 369 370 371 372 484 485 486 487 488 489 490 491 492 493 494 495\n",
            " 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513\n",
            " 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 194 195 196 197 198 199 200 201 202 203 204 205\n",
            " 206 207 208 209 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
            " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
            " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
            " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
            " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
            " 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n",
            " 329 330 331 332 333 334 335 336 337 338 339 340 341 342 353 354 355 356\n",
            " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
            " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
            " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
            " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428\n",
            " 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446\n",
            " 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n",
            " 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n",
            " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500\n",
            " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
            " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536\n",
            " 537 538 539 540 541 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
            " 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581\n",
            " 582 583 584 585 586 587 588 589 590] TEST: [150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n",
            " 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
            " 186 187 188 189 190 191 192 193 210 211 212 213 214 215 216 217 218 219\n",
            " 220 221 222 223 224 343 344 345 346 347 348 349 350 351 352 542 543 544\n",
            " 545 546 547 548 549 550]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 333\n",
            " 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351\n",
            " 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369\n",
            " 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387\n",
            " 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405\n",
            " 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423\n",
            " 424 425 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
            " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
            " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
            " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
            " 554 555 556 557 558 559 560 571 572 573 574 575 576 577 578 579 580 581\n",
            " 582 583 584 585 586 587 588 589 590] TEST: [323 324 325 326 327 328 329 330 331 332 426 427 428 429 430 431 432 433\n",
            " 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451\n",
            " 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469\n",
            " 470 471 472 473 474 475 476 477 478 479 480 481 482 483 561 562 563 564\n",
            " 565 566 567 568 569 570]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 210 211 212 213\n",
            " 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
            " 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249\n",
            " 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
            " 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285\n",
            " 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
            " 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321\n",
            " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 383 384 385\n",
            " 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403\n",
            " 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421\n",
            " 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439\n",
            " 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457\n",
            " 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475\n",
            " 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493\n",
            " 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511\n",
            " 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529\n",
            " 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547\n",
            " 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565\n",
            " 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580] TEST: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\n",
            " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149\n",
            " 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 373 374\n",
            " 375 376 377 378 379 380 381 382 581 582 583 584 585 586 587 588 589 590]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 241 242 243 244 245 246 247 248 249\n",
            " 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
            " 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285\n",
            " 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
            " 304 305 306 307 308 309 310 311 312 323 324 325 326 327 328 329 330 331\n",
            " 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349\n",
            " 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
            " 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [ 78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
            "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
            " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 313 314\n",
            " 315 316 317 318 319 320 321 322 383 384 385 386 387 388 389 390 391 392]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 426 427 428\n",
            " 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446\n",
            " 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n",
            " 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n",
            " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500\n",
            " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
            " 519 520 521 522 523 524 525 526 527 528 529 530 542 543 544 545 546 547\n",
            " 548 549 550 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590] TEST: [ 60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
            " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 531 532 533\n",
            " 534 535 536 537 538 539 540 541 551 552 553 554 555 556 557 558 559 560]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
            " 328 329 330 331 332 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
            " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
            " 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391\n",
            " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
            " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
            " 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445\n",
            " 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463\n",
            " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481\n",
            " 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
            " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
            " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
            " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
            " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 581\n",
            " 582 583 584 585 586 587 588 589 590] TEST: [255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
            " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
            " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
            " 309 310 311 312 333 334 335 336 337 338 339 340 341 342 571 572 573 574\n",
            " 575 576 577 578 579 580]\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
            " 328 329 330 331 332 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
            " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
            " 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391\n",
            " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
            " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
            " 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445\n",
            " 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463\n",
            " 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481\n",
            " 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
            " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
            " 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
            " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
            " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 581\n",
            " 582 583 584 585 586 587 588 589 590] [255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
            " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
            " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
            " 309 310 311 312 333 334 335 336 337 338 339 340 341 342 571 572 573 574\n",
            " 575 576 577 578 579 580]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwJbWuLsYSev",
        "outputId": "ce50dfb8-9dbd-40ea-e217-6cef69811b6d"
      },
      "source": [
        "# Fine-tune CNN\r\n",
        "epochs = 30 # Change this number to ~30 for full training\r\n",
        "batch_size = 10\r\n",
        "t0 = time()\r\n",
        "# Set up Adam optimizer\r\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\r\n",
        "# Set up dataloaders\r\n",
        "dl_tr = spectral_dataloader(X, Y, idxs=train_index,\r\n",
        "    batch_size=batch_size, shuffle=True)\r\n",
        "dl_val = spectral_dataloader(X, Y, idxs=test_index,\r\n",
        "    batch_size=batch_size, shuffle=False)\r\n",
        "# Fine-tune CNN for first fold\r\n",
        "best_val = 0\r\n",
        "no_improvement = 0\r\n",
        "max_no_improvement = 5\r\n",
        "print('Starting fine-tuning!')\r\n",
        "for epoch in range(epochs):\r\n",
        "    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\r\n",
        "    # Train\r\n",
        "    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\r\n",
        "        training=True, optimizer=optimizer)\r\n",
        "    print('  Train acc: {:0.2f}'.format(acc_tr))\r\n",
        "    # Val\r\n",
        "    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\r\n",
        "        training=False, optimizer=optimizer)\r\n",
        "    print('  Val acc  : {:0.2f}'.format(acc_val))\r\n",
        "    # Check performance for early stopping\r\n",
        "    if acc_val > best_val or epoch == 0:\r\n",
        "        best_val = acc_val\r\n",
        "        no_improvement = 0\r\n",
        "    else:\r\n",
        "        no_improvement += 1\r\n",
        "    if no_improvement >= max_no_improvement:\r\n",
        "        print('Finished after {} epochs!'.format(epoch+1))\r\n",
        "        break\r\n",
        "\r\n",
        "print('\\n This demo was completed in: {:0.2f}s'.format(time()-t00))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting fine-tuning!\n",
            " Epoch 1: 0.00s\n",
            "  Train acc: 99.81\n",
            "  Val acc  : 58.97\n",
            " Epoch 2: 2.22s\n",
            "  Train acc: 100.00\n",
            "  Val acc  : 58.97\n",
            " Epoch 3: 4.44s\n",
            "  Train acc: 100.00\n",
            "  Val acc  : 58.97\n",
            " Epoch 4: 6.50s\n",
            "  Train acc: 100.00\n",
            "  Val acc  : 58.97\n",
            " Epoch 5: 8.61s\n",
            "  Train acc: 100.00\n",
            "  Val acc  : 58.97\n",
            " Epoch 6: 10.80s\n",
            "  Train acc: 100.00\n",
            "  Val acc  : 58.97\n",
            "Finished after 6 epochs!\n",
            "\n",
            " This demo was completed in: 521.50s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}